import torch


x = torch.tensor(3.0, requires_grad = True)


x


y = x**2



y


y.backward()


x.grad


x = torch.tensor(3.0, requires_grad=True)



y =  x ** 2



z = torch.sin(y)


x


y


z



z.backward()


x.grad


y.grad





x = torch.tensor(6.7)
y = torch.tensor(0.0)

w = torch.tensor(1.0)
b = torch.tensor(0.0)


def binary_cross_entropy(prediction, target):
  epsilon = 1e-8    #to prevent log(0)
  prediction = torch.clamp(prediction, epsilon, 1-epsilon)
  return -(target * torch.log(prediction) + (1 - target) * torch.log(1-prediction))



#forward pass

z = w*x + b   #weighted sum (linear part)
y_pred = torch.sigmoid(z)

loss = binary_cross_entropy(y_pred, y)


loss


#Derivatives:
#1. dL/d(y_pred) : Loss with respect to the prediction (ypred)
dloss_dy_pred = (y_pred - y)/(y_pred*(1-y_pred))

#2. dy_pred/dz : prediction (ypred) with respect to z (sigmoid derivative)
dy_pred_dz = y_pred * (1 - y_pred)

#3. dz/dw and dz/db : z with respect to w and b
dz_dw = x
dz_db = 1

dL_dw = dloss_dy_pred * dy_pred_dz * dz_dw
dL_db = dloss_dy_pred * dy_pred_dz * dz_db



print(f"Manual Gradient of loss w.r.t weight (dw) : {dL_dw}" )
print(f"Manual Gradient of loss w.r.t bias (dw) : {dL_db}" )


x = torch.tensor(6.7)
y = torch.tensor(0.0)


w = torch.tensor(1.0, requires_grad=True)
b = torch.tensor(0.0, requires_grad=True)


w


b


z = w*x + b


z


y_pred = torch.sigmoid(z)


y_pred


loss = binary_cross_entropy(y_pred, y)


loss


loss.backward()


print(w.grad)
print(b.grad)


x = torch.tensor([1.0, 2.0, 3.0], requires_grad= True)


x


y = (x**2).mean(0)


y


y.backward()


x.grad


#clearing gradients
x = torch.tensor(2.0, requires_grad=True)


y = x**2
y



# y.backward()


x.grad


x.requires_grad_(False)


# # how to desable gradient tracking
# requires_grad = False
# detach()
# torch.no_grad()


x = torch.tensor(2.0, requires_grad=True)


x


# z = x.detach()


# y = x**2


with torch.no_grad():
  y = x**2


y


# y.backward()


x = torch.tensor(4.0, requires_grad=True)


y = x**3


y.backward()


x.grad


x.grad.zero_()





x = torch.tensor(2.0, requires_grad=True)


y = x**3


grad1 = torch.autograd.grad(y, x, retain_graph = True, create_graph = True)
grad1


grad2 = torch.autograd.grad(grad1[0], x)
print(grad2)


x = (3,)


print(x)


x


print(x[0])



